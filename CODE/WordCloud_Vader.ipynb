{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcQHrgBLWxe9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3OfylEAQ5Xz",
        "outputId": "af8ed17a-7011-4ea2-94e3-42fda84524ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7LI3FatRQvH",
        "outputId": "51301ca5-34b3-4c37-eef5-084ce402d46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "MtXM9DIjy-hd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f59c7d-2f6e-4c94-c088-10e749ca20c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sid = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "BbvwiLVqybd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://insideairbnb.com/get-the-data/'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')"
      ],
      "metadata": {
        "id": "-ZpkJ073W7pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping\n",
        "body = soup.find('body', class_='layout-module--content--2bfc1')\n",
        "div = body.find('div', {\"id\": \"___gatsby\"})\n",
        "file_urls = div.find_all('a')\n",
        "\n",
        "links = [link.get('href') for link in file_urls]\n",
        "\n",
        "listings_list = []\n",
        "reviews_list = []\n",
        "for link in links:\n",
        "    if link != None:\n",
        "        if \"united-states\" in link and \"listings.csv.gz\" in link:\n",
        "            listings_list.append(link)\n",
        "\n",
        "        if \"united-states\" in link and \"reviews.csv.gz\" in link:\n",
        "            reviews_list.append(link)\n"
      ],
      "metadata": {
        "id": "LsY5AUPjXGsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Running vader on each comment\n",
        "big_df = pd.DataFrame()\n",
        "count = len(reviews_list)\n",
        "for i,review in zip(range(count),reviews_list):\n",
        "\n",
        "  reviews_df = pd.read_csv(review)\n",
        "  reviews_df = reviews_df[['listing_id','comments']]\n",
        "  reviews_df = reviews_df.rename(columns={'listing_id':'id'})\n",
        "  reviews_df['comments'] = reviews_df['comments'].replace('[^a-zA-Z\\s]', '', regex=True)\n",
        "\n",
        "\n",
        "  ### vader\n",
        "  reviews_df['sentiment_score'] = reviews_df['comments'].apply(lambda comments:sid.polarity_scores(str(comments))['compound'])\n",
        "  big_df = big_df.append(reviews_df)\n",
        "  print(\"Complete\",i,\"of\",count, review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NopfRdmjXpn1",
        "outputId": "b077b510-691d-4708-d5a7-f82fe45b1e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete 0 of 1 http://data.insideairbnb.com/united-states/ny/albany/2023-10-01/data/reviews.csv.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-35205dc31321>:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  big_df = big_df.append(reviews_df)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove stop words\n",
        "english_stop_words = stopwords.words('english')\n",
        "def remove_stop_words(review):\n",
        "    if isinstance(review, str):\n",
        "      removed_stop_words = [word for word in review.split() if word.lower() not in english_stop_words]\n",
        "      return removed_stop_words\n",
        "    else:\n",
        "      return"
      ],
      "metadata": {
        "id": "Z2Aunhu2RBqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for getting frequency of each word\n",
        "from collections import Counter\n",
        "def word_counter(word_list):\n",
        "  counts = Counter(word_list)\n",
        "  return(counts)"
      ],
      "metadata": {
        "id": "B3pdp0qtRfXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get 50 most common words\n",
        "def top_word(word_counter):\n",
        "  return(word_counter.most_common(50))"
      ],
      "metadata": {
        "id": "IwprzCdrRil3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seggragating comments on the basis of sentiment score\n",
        "big_df.loc[big_df['sentiment_score'] < 0, 'type_of_comm'] = 'negative'\n",
        "big_df.loc[big_df['sentiment_score'] >= 0, 'type_of_comm'] = 'positive'"
      ],
      "metadata": {
        "id": "C0fGMy5hRu92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the comments column\n",
        "big_df['comments'] = big_df['comments'].astype(\"string\")\n",
        "big_df['comments'] = big_df['comments'] .fillna(\" \")"
      ],
      "metadata": {
        "id": "MdWEPfKmR97J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_df_comments = big_df.groupby(['id','type_of_comm'], as_index=False).agg({'comments': ' '.join})"
      ],
      "metadata": {
        "id": "VmgfHMzMRh9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_df_comments[\"word_list\"] = big_df_comments['comments'].apply(remove_stop_words)"
      ],
      "metadata": {
        "id": "k0nKB7R3TFN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_df_comments[\"word_counter\"] = big_df_comments['word_list'].apply(word_counter)"
      ],
      "metadata": {
        "id": "jsb7QtgsTMWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_df_comments[\"top_word\"] = big_df_comments['word_counter'].apply(top_word)"
      ],
      "metadata": {
        "id": "08WBXH7JTM6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_df_comments['top_word'] = big_df_comments['top_word'].astype(\"string\")\n"
      ],
      "metadata": {
        "id": "haTtSbqXqJM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_df_comments=big_df_comments[['id','top_word','type_of_comm']]\n",
        "\n",
        "# Extracting the list from the string\n",
        "big_df_comments['Column2'] = big_df_comments['top_word'].apply(lambda x: eval(x))\n",
        "\n",
        "expanded = big_df_comments['Column2'].explode().apply(pd.Series)\n",
        "\n",
        "result_df = (\n",
        "    pd.concat([big_df_comments.drop('Column2', axis=1), expanded.rename(columns={0: 'Column3', 1: 'Column4'})], axis=1)\n",
        "    .reset_index(drop=True)\n",
        ")"
      ],
      "metadata": {
        "id": "-xQ0MCWU2dzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1361d89-f4f1-4456-80df-051d05abf59c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-8996e47fef1f>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  big_df_comments['Column2'] = big_df_comments['top_word'].apply(lambda x: eval(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final dataframe with id of listing and the top positive and negative words\n",
        "result_df = result_df[[\"id\", \"type_of_comm\", \"Column3\", \"Column4\"]]"
      ],
      "metadata": {
        "id": "J8tZk8ZtMofG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}